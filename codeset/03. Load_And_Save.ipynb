{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "215b7ca2-b9ef-47be-bd16-a69d703a99b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import google.colab\n",
    "    inColab = True\n",
    "except ImportError:\n",
    "    inColab = False\n",
    "\n",
    "if inColab == True:\n",
    "    !pip install -U pandas==2.2.2 scipy==1.14.1 accelerate==1.6.0 peft==0.15.2 bitsandbytes==0.45.5 transformers==4.51.3 trl==0.16.1 datasets==3.5.0 tensorboard==2.19.0\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    logging,\n",
    "    GenerationConfig\n",
    ")\n",
    "from transformers import AutoConfig,AutoModel\n",
    "import torch\n",
    "from peft import PeftModel, PeftConfig\n",
    "\n",
    "if inColab == True:\n",
    "    from google.colab import drive\n",
    "    drive.mount(\"/content/gdrive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16557490-b453-4754-9b53-a1f2b865b59c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cbe2bee2e624d9e91b992b88bb9d863",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import huggingface_hub\n",
    "huggingface_hub.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a22c7119-1a1b-453a-b405-dc7818017fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # \"google/gemma-7b-it\",\n",
    "    # torch_dtype=torch.bfloat16,  # 또는 torch.float32\n",
    "    # device_map=\"auto\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "184297f3-5f0b-490d-bcc8-0e9854ec9771",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5047cf577d94279a652d7a9d8668d4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoraConfig(task_type='CAUSAL_LM', peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path='google/gemma-3-4b-it', revision=None, inference_mode=True, r=64, target_modules={'down_proj', 'up_proj', 'gate_proj', 'o_proj', 'k_proj', 'q_proj', 'v_proj'}, exclude_modules=None, lora_alpha=16, lora_dropout=0.1, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', trainable_token_indices=None, loftq_config={}, eva_config=None, corda_config=None, use_dora=False, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"# 1. Load base and new model\"\"\"\n",
    "\n",
    "## base 모델\n",
    "# base_model =  \"google/gemma-2b-it\"\n",
    "base_model = \"google/gemma-3-4b-it\"\n",
    "## 파인튜닝한 모델 저장 위치 설정\n",
    "\n",
    "# new_model = \"/content/drive/MyDrive/Colab Notebooks/models/gemma_tune_2025_06_08_10\"\n",
    "new_model = \"./models/gemma_tune_2025_07_18_16\"\n",
    "### 베이스모델 불러오기\n",
    "baseModel = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    low_cpu_mem_usage=True,\n",
    "    return_dict=True,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map= \"auto\"\n",
    ")\n",
    "\n",
    "### 토크나이저 불러오기\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "# PEFT 설정 로드\n",
    "peft_config = PeftConfig.from_pretrained(new_model)\n",
    "print(peft_config)\n",
    "\n",
    "# 베이스모델에 어댑터 적용\n",
    "model = PeftModel.from_pretrained(baseModel, new_model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b0167ce-447f-4230-8d15-b6122e758d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시스템 메시지 예시\n",
    "DEFAULT_SYSTEM_MESSAGE = \"당신은 전문의 수준의 의료 문제를 정확하게 답변하는 AI입니다.\"\n",
    "DEVICE = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "897e0ff4-1be8-418a-b634-20d7cb5e5071",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시스템 메시지 + 문제(질문) 기반 답변 생성 함수\n",
    "\n",
    "def generate_gemma_answer(model, tokenizer, user_message, system_message=None, device=DEVICE,\n",
    "                          max_new_tokens=512, temperature=0.2, top_p=0.95, top_k=50):\n",
    "    if system_message:\n",
    "        prompt = (\n",
    "            f\"<start_of_turn>system\\n{system_message}<end_of_turn>\\n\"\n",
    "            f\"<start_of_turn>user\\n{user_message}<end_of_turn>\\n\"\n",
    "            f\"<start_of_turn>model\\n\"\n",
    "        )\n",
    "    else:\n",
    "        prompt = (\n",
    "            f\"<start_of_turn>user\\n{user_message}<end_of_turn>\\n\"\n",
    "            f\"<start_of_turn>model\\n\"\n",
    "        )\n",
    "    inputs = tokenizer(prompt, return_tensors='pt').to(device)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=True,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            top_k=top_k,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "        )\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bc039b2a-b6d7-4e6e-a107-65215e953e9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[질문]\n",
      " 23세 여자가 3개월 전부터 기침을 한다며 내원했다. 기침은 밤에 누워 자려고 할 때 심해진다고 한다. 1년 전에도 같은 시기에 기침이 3개월 동안 지속되다가 저절로 호전된 병력이 있다. 콧물이나 인후부 불편감은 없으며, 비흡연자이고 복용 중인 약물도 없다. 신체검사에서 혈압 120/80mmHg, 맥박 78회/분, 호흡 18회/분, 체온 36.5°C로 측정되었다. 청진상 호흡음은 정상이었고, 가슴 X선 사진과 코곁굴 X선 사진에서도 이상 소견이 없었다. 폐기능검사 결과는 다음과 같다.\n",
      "- 강제 폐활량(FVC): 정상 예측치의 91%\n",
      "- 1초간 강제날숨유량(FEV1): 정상 예측치의 85%\n",
      "- 1초간 강제날숨유량/강제폐활량(FEV1/FVC): 75%\n",
      "\n",
      "이 환자에서 다음으로 시행해야 할 검사는 무엇인가?\n",
      "1) 기관지내시경\n",
      "2) 기관지 확장제 반응 검사\n",
      "3) 가슴 컴퓨터단층촬영(CT)\n",
      "4) 메타콜린 기관지유발검사\n",
      "5) 폐 확산능 검사\n",
      "\n",
      "[답변]\n",
      " system\n",
      "당신은 전문의 수준의 의료 문제를 정확하게 답변하는 AI입니다.\n",
      "user\n",
      "23세 여자가 3개월 전부터 기침을 한다며 내원했다. 기침은 밤에 누워 자려고 할 때 심해진다고 한다. 1년 전에도 같은 시기에 기침이 3개월 동안 지속되다가 저절로 호전된 병력이 있다. 콧물이나 인후부 불편감은 없으며, 비흡연자이고 복용 중인 약물도 없다. 신체검사에서 혈압 120/80mmHg, 맥박 78회/분, 호흡 18회/분, 체온 36.5°C로 측정되었다. 청진상 호흡음은 정상이었고, 가슴 X선 사진과 코곁굴 X선 사진에서도 이상 소견이 없었다. 폐기능검사 결과는 다음과 같다.\n",
      "- 강제 폐활량(FVC): 정상 예측치의 91%\n",
      "- 1초간 강제날숨유량(FEV1): 정상 예측치의 85%\n",
      "- 1초간 강제날숨유량/강제폐활량(FEV1/FVC): 75%\n",
      "\n",
      "이 환자에서 다음으로 시행해야 할 검사는 무엇인가?\n",
      "1) 기관지내시경\n",
      "2) 기관지 확장제 반응 검사\n",
      "3) 가슴 컴퓨터단층촬영(CT)\n",
      "4) 메타콜린 기관지유발검사\n",
      "5) 폐 확산능 검사\n",
      "model\n",
      "4) 메타콜린 기관지유발검사\n"
     ]
    }
   ],
   "source": [
    "# 문제(질문) 예시\n",
    "user_message = (\n",
    "    \"23세 여자가 3개월 전부터 기침을 한다며 내원했다. 기침은 밤에 누워 자려고 할 때 심해진다고 한다. \"\n",
    "    \"1년 전에도 같은 시기에 기침이 3개월 동안 지속되다가 저절로 호전된 병력이 있다. 콧물이나 인후부 불편감은 없으며, \"\n",
    "    \"비흡연자이고 복용 중인 약물도 없다. 신체검사에서 혈압 120/80mmHg, 맥박 78회/분, 호흡 18회/분, 체온 36.5°C로 측정되었다. \"\n",
    "    \"청진상 호흡음은 정상이었고, 가슴 X선 사진과 코곁굴 X선 사진에서도 이상 소견이 없었다. 폐기능검사 결과는 다음과 같다.\\n\"\n",
    "    \"- 강제 폐활량(FVC): 정상 예측치의 91%\\n\"\n",
    "    \"- 1초간 강제날숨유량(FEV1): 정상 예측치의 85%\\n\"\n",
    "    \"- 1초간 강제날숨유량/강제폐활량(FEV1/FVC): 75%\\n\\n\"\n",
    "    \"이 환자에서 다음으로 시행해야 할 검사는 무엇인가?\\n\"\n",
    "    \"1) 기관지내시경\\n\"\n",
    "    \"2) 기관지 확장제 반응 검사\\n\"\n",
    "    \"3) 가슴 컴퓨터단층촬영(CT)\\n\"\n",
    "    \"4) 메타콜린 기관지유발검사\\n\"\n",
    "    \"5) 폐 확산능 검사\"\n",
    ")\n",
    "# 시스템 메시지(옵션)\n",
    "system_message = DEFAULT_SYSTEM_MESSAGE\n",
    "# 답변 생성\n",
    "output = generate_gemma_answer(model, tokenizer, user_message, system_message)\n",
    "print(\"\\n[질문]\\n\", user_message)\n",
    "print(\"\\n[답변]\\n\", output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "91c2ff60-5ad3-4bfe-8e15-873e98e056ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdf9922247224bd9bdf86409fbed963a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/31.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\.finetune\\Lib\\site-packages\\huggingface_hub\\file_download.py:144: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\HP\\.cache\\huggingface\\hub\\models--hyokwan--familidata_elysium. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7c49bfa20a04df69b86661b618dddef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 2 LFS files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13775b82962e42578bc21ddc35de4731",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.96G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8c469f265dd4e1db57d15af78a9566d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/3.64G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22507e5fb6294efb8b983486653ddafc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/31.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7d4d2da93144f0789a515f2702a4d2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/33.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/hyokwan/familidata_elysium/commit/42aaad27cf770bd3527eab497534e284f8f5b7b2', commit_message='Upload tokenizer', commit_description='', oid='42aaad27cf770bd3527eab497534e284f8f5b7b2', pr_url=None, repo_url=RepoUrl('https://huggingface.co/hyokwan/familidata_elysium', endpoint='https://huggingface.co', repo_type='model', repo_id='hyokwan/familidata_elysium'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 어댑터 병합\n",
    "mergedModel = model.merge_and_unload()\n",
    "\n",
    "# set your HF repository\n",
    "hfAddr = \"hyokwan/familidata_elysium\"\n",
    "\n",
    "# save model and tokenizer to HF hub\n",
    "mergedModel.push_to_hub(hfAddr)\n",
    "tokenizer.push_to_hub(hfAddr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b421398-361f-4219-8afe-cf64882a16e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
