{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7707b801-0b0a-432a-a977-bc80256da68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da69bfc7-4299-4128-a4cb-6b231f8997e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/gdrive')\n",
    "    import google.colab\n",
    "    inColab = True\n",
    "except ImportError:\n",
    "    inColab = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1d22c42-64c1-4b50-beb1-40f780036f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "if inColab == True:\n",
    "    !pip install -U pandas==2.2.2 numpy==2.0.2 scipy==1.14.1 accelerate==1.6.0 peft==0.15.2 bitsandbytes==0.45.5 transformers==4.51.3 trl==0.16.1 datasets==3.5.0 tensorboard==2.19.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "09dfb06a-b03d-43a0-9a3e-0ba5531507cb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    logging,\n",
    ")\n",
    "from peft import LoraConfig\n",
    "from trl import SFTTrainer\n",
    "from datetime import datetime\n",
    "import huggingface_hub\n",
    "\n",
    "torch.cuda.is_available()\n",
    "\n",
    "if inColab == True:\n",
    "    from google.colab import drive\n",
    "    drive.mount(\"/content/gdrive\")\n",
    "\n",
    "# huggingface_hub.login()\n",
    "\n",
    "\"\"\"# 1. set model and dataset\n",
    "\n",
    "### ★★★ 수정 포인트 ★★★\n",
    "\"\"\"\n",
    "# base_model = \"google/gemma-2b-it\"\n",
    "base_model = \"google/gemma-3-4b-it\"\n",
    "dataset_name = \"hyokwan/familicare_health_general_knowledge\"\n",
    "datasetCommon = load_dataset(dataset_name, split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e9178df9-3f5d-4d21-9415-fdb5b8c39034",
   "metadata": {},
   "outputs": [],
   "source": [
    "# datasetCommon.push_to_hub(\"hyokwan/senior_sleeping\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "46459d38-5198-4155-a538-294c211f6f2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instruction': '58세 남자가 3시간 전부터 시작된 토혈로 응급실에 내원하였다. 환자는 5년 전 간경화증으로 진단받아 항바이러스제를 복용 중이다. 활력징후는 혈압 115/80mmHg, 맥박 85회/분, 호흡 18회/분, 체온 36.5°C이다. 신체 진찰에서 양쪽 팔다리에 여러 개의 멍이 관찰되었다. 검사 결과는 다음과 같다. 이 환자에게 가장 적절한 치료는 무엇인가?  \\n혈액검사: 혈색소 10.3g/dL, 백혈구 3,200/mm³, 혈소판 90,000/mm³, 프로트롬빈시간 42초 (참고치: 12.7~15.4초), 활성화부분트롬보플라스틴시간 68초 (참고치: 26.3~39.4초) \\n\\n1) 비타민 K 근육주사  \\n2) VIII인자 정맥주사  \\n3) 혈소판농축물 수혈  \\n4) 농축적혈구 수혈  \\n5) 신선냉동혈장 수혈',\n",
       " 'input': '내과',\n",
       " 'output': '5) 신선냉동혈장 수혈'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasetCommon\n",
    "\n",
    "datasetCommon[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0916346f-a44b-479b-851b-13f7360c5468",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtered_dataset = datasetCommon.filter(lambda x: x[\"input\"] == \"내과\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bfaf00e4-520c-477e-8e04-563f315feac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetCommon = datasetCommon.filter(lambda x: x[\"input\"] == \"내과\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "deed6623-1b9a-4a8b-ae05-16966dfd2631",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetCommon = datasetCommon.select(range(min(100, len(datasetCommon))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1876c7c9-642b-4480-9b7b-bdcfa45e7b47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 3)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasetCommon.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "039ce744-c08c-4187-86e9-2d0288848c4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': '<start_of_turn>user\\n23세 여자가 3개월 전부터 기침을 한다며 내원했다. 기침은 밤에 누워 자려고 할 때 심해진다고 한다. 1년 전에도 같은 시기에 기침이 3개월 동안 지속되다가 저절로 호전된 병력이 있다. 콧물이나 인후부 불편감은 없으며, 비흡연자이고 복용 중인 약물도 없다. 신체검사에서 혈압 120/80mmHg, 맥박 78회/분, 호흡 18회/분, 체온 36.5°C로 측정되었다. 청진상 호흡음은 정상이었고, 가슴 X선 사진과 코곁굴 X선 사진에서도 이상 소견이 없었다. 폐기능검사 결과는 다음과 같다.  \\n- 강제 폐활량(FVC): 정상 예측치의 91%  \\n- 1초간 강제날숨유량(FEV1): 정상 예측치의 85%  \\n- 1초간 강제날숨유량/강제폐활량(FEV1/FVC): 75%  \\n\\n이 환자에서 다음으로 시행해야 할 검사는 무엇인가?  \\n1) 기관지내시경  \\n2) 기관지 확장제 반응 검사  \\n3) 가슴 컴퓨터단층촬영(CT) \\n4) 메타콜린 기관지유발검사  \\n5) 폐 확산능 검사\\n내과<end_of_turn>\\n<start_of_turn>model\\n4) 메타콜린 기관지유발검사<end_of_turn>'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasetCommon[0]\n",
    "\n",
    "def convert_to_gemma_format(dataset):\n",
    "    instruction = dataset['instruction']\n",
    "    input=dataset['input']\n",
    "    output = dataset['output']\n",
    "    text = f\"<start_of_turn>user\\n{instruction}\\n{input}<end_of_turn>\\n<start_of_turn>model\\n{output}<end_of_turn>\"\n",
    "\n",
    "    #text = f\"<start_of_turn>{instruction}<end_of_turn>{input}<start_of_turn>{output}<end_of_turn>\"\n",
    "    dataset[\"text\"] = text\n",
    "    return dataset\n",
    "\n",
    "datasetCommon = datasetCommon.map(convert_to_gemma_format,remove_columns=['instruction', 'input', 'output'])\n",
    "\n",
    "print(datasetCommon[0])\n",
    "\n",
    "datasetCommon\n",
    "\n",
    "datasetCommon[10]\n",
    "\n",
    "\"\"\"# 2. Config efficient fine-tuning with low-rank adaptation.\"\"\"\n",
    "torch.cuda.get_device_capability()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0debdf0-b33e-4ace-a8bf-7630238d2c59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8842f76931cc4213a9516473926b1948",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 현재 사용 중인 GPU의 주요 아키텍처 버전을 반환 8버전 이상 시 bfloat16 활용\n",
    "# NVIDIA Ampere 아키텍처 이상 시에만 처리\n",
    "# 정확도를 위하여 float 16이 타도록 강제설정 8->10 ★ 향후 변경 필요\n",
    "if torch.cuda.get_device_capability()[0] >= 8:\n",
    "    # 고속 attention 메커니즘을 구현하는 라이브러리\n",
    "    attn_implementation = \"flash_attention_2\"\n",
    "    torch_dtype = torch.bfloat16\n",
    "else:\n",
    "    attn_implementation = \"eager\"\n",
    "    torch_dtype = torch.float16\n",
    "\n",
    "\n",
    "# BitsAndBytesConfig 객체활용 양자화 설정\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    # 모델을 4비트 양자화하여 로드할지 여부 결정\n",
    "    load_in_4bit=True,\n",
    "    # 양자화 방법 (nf4: Non-Uniform Quantization, \"nf4\",\"fp16 등))\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    # (4비트 양자화 시 사용할 데이터 타입, \"torch.float16, bfloat16, float32 등)\n",
    "    bnb_4bit_compute_dtype=torch_dtype,\n",
    "    # 이중 양자화 사용여부 (이중 양자화는 양자화 과정에서 정밀도 높이기 위해 활용, 대신 더 연산은 복잡)\n",
    "    bnb_4bit_use_double_quant=False\n",
    ")\n",
    "\n",
    "\"\"\"# 3. Load Pre-trained Language Model\"\"\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    # 불러올 모델 정의\n",
    "    base_model,\n",
    "    # 모델 양자화 설정값\n",
    "    quantization_config=quant_config,\n",
    "    # 모델의 레이어를 할당할 장치 (\"\":0 -> 전체 모델을 GPU 0에 할당, \"auto\"는 알아서, \"{\"layer_0\":0, ... 형태로 레이어별 할당 가능)\n",
    "    # device_map={\"\": 0}\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "# 캐시 사용 여부 (모델 출력 매번 새로 계산)\n",
    "# 새로운 데이터에 대한 계산 시 저장하는게 불필요함 또한 True 시 캐시 저장할 메모리 사용하여 메모리 사용량 증가함\n",
    "model.config.use_cache = False\n",
    "# 모델읜 pretraining tensor parallelism 설정 1인경우 병렬처리 안하고 단일장치 활용 2이상 시 병렬 처리를 분산 가능\n",
    "# 병렬 처리 시 추가적인 메모리 오버헤드 발생 가능함\n",
    "model.config.pretraining_tp = 1\n",
    "\n",
    "\"\"\"# 4. Load Pre-trained Language Model Tokenizer\"\"\"\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)\n",
    "\n",
    "# ★★★ 수정포인트 ★★★\n",
    "# Load tokenizer ( tokenizer.model 파일 생성됨, 단, pip install sentencepiece 설치 <-- GGUF 파일 생성 시 필요)\n",
    "# tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True, use_fast=False)\n",
    "\n",
    "# Must add EOS_TOKEN at response last line\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# ★수정 포인트!!! 기존 # tokenizer.padding_side = \"right\"\n",
    "EOS_TOKEN = tokenizer.eos_token\n",
    "def prompt_eos(sample):\n",
    "    sample['text'] = sample['text']+EOS_TOKEN\n",
    "    return sample\n",
    "datasetCommon = datasetCommon.map(prompt_eos)\n",
    "\n",
    "# datasetCommon[199]\n",
    "\n",
    "import gc\n",
    "# # Flush memory\n",
    "# del trainer, model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4745f9b0-f223-457d-bcb2-33ef59067dce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.bfloat16"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e2389a51-266e-445f-98a4-bcfaeeaedf8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "It is strongly recommended to train Gemma3 models with the `eager` attention implementation instead of `sdpa`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.\n",
      "C:\\Users\\HP\\.finetune\\Lib\\site-packages\\transformers\\integrations\\sdpa_attention.py:54: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:263.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [500/500 06:21, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>1.824400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.308700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>1.212300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.195600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>0.749800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.880300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>0.807400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.809400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>225</td>\n",
       "      <td>0.476900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.534500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>275</td>\n",
       "      <td>0.454900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.521100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>325</td>\n",
       "      <td>0.306000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.315600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>375</td>\n",
       "      <td>0.358200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.355000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>425</td>\n",
       "      <td>0.224200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.237900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>475</td>\n",
       "      <td>0.263700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.251700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=500, training_loss=0.6543735513687133, metrics={'train_runtime': 383.4968, 'train_samples_per_second': 1.304, 'train_steps_per_second': 1.304, 'total_flos': 2973379152352800.0, 'train_loss': 0.6543735513687133})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"# 5. Config training parameter for LoRA (Parameter-Efficient Fine-Tuning (PEFT)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "peft_params = LoraConfig(\n",
    "    # lora 방식의 튜닝에서 저차원 병렬 레이어 학습 ( 값이 크면 저차원 병렬 레이어 영향이 커지고 작으면 줄어듬, 기본 16, 1~128)\n",
    "    lora_alpha=16,\n",
    "    # 과적합 방지 (0.1에서 증가 시킴, 0.0~0.3)\n",
    "    lora_dropout=0.1,\n",
    "    # LoRA 저차원 공간의 차원 수 (값이 커지면 파라미터 공간 커지지만 계산비용 증가 4, 8, 16, 32, 64 로 튜닝, 4~128)\n",
    "    r=64,\n",
    "    # LoRA 모델의 바이어스 파라미터 적용여부 None = 미적용 (\"none\" or \"all\", \"lora_only\" 설정 가능)\n",
    "    bias=\"none\",\n",
    "    # LoLA 모델의 작업 유형 (GPT와 같은 인과모델) (\"CASUAL_LM\": GPT, \"SEQ_CLASSSIFICATION\": 시퀀스 분류, \"TOKEN_CLASSIFICATION\": 토큰 분류, \"SEQ_2_SEQ_LM\": 번역모델)\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "\n",
    "    target_modules=[\n",
    "           \"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\",\n",
    "            \"gate_proj\", \"up_proj\", \"down_proj\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "training_params = TrainingArguments(\n",
    "    # 학습 결과와 체크포인트를 저장할 디렉토리 경로\n",
    "    output_dir= \"./results\",\n",
    "\n",
    "\n",
    "    # 전체 데이터셋을 반복학습 주기 (1~50 정도 활용)\n",
    "    num_train_epochs=5,\n",
    "    # 각 GPU 장치에서 사용하는 배치 크기 (배치 크기에 따라 가중치 업데이트 1~64)\n",
    "    # 배치크기는 모델히 한번에 처리하는 데이터 샘플의 수\n",
    "    per_device_train_batch_size=1,\n",
    "    #그래디언트 누적을 통한 배치 크기 증가 (여러 작은 배치를 합쳐서 효과적으로 큰 배치 크기를 적용 가능) *4 적용 시 그래디언트 누적 4개 후 한번에 가중치 업데이트\n",
    "    # 배치크기가 4배 증가한 효과를 가지지만 실제 메모리 사용은 배치 1개 크기만 사용\n",
    "    gradient_accumulation_steps=1,\n",
    "    # 사용할 옵티마이저 설정 (\"adamw, paged_adamw_32bit\",\"adamw_torch\")\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    # 학습 중 모델 체크포인트 저장 주기 (스템 수)\n",
    "    save_steps=25,\n",
    "    # 학습 중 로그를 기록할 주기(스텝 수)\n",
    "    logging_steps=25,\n",
    "    # 학습률 설정 (모델의 파라미터 업데이트 시 스템 크기, 1e-6 ~ 1e-2))\n",
    "    learning_rate=2e-4,\n",
    "    # 모델의 가중치에 패널티 적용하여 과적합 방지 (0.0은 정규화 안함, 0.0 ~ 0.1)\n",
    "    weight_decay=0.001,\n",
    "    # 16비트 부동 소수점(FP16) 연산을 사용하여 메모리 사용량과 계산속도 개선 (사용시 정밀도 감소, 32비트 부동소수점보다 절반)\n",
    "    fp16=False,\n",
    "    # bfloat16 연산을 사용하여 학습 FP16대비 더 넓은 지수 범위 (사용시 정밀도 감소, 32비트 부동소수점보다 절반)\n",
    "    bf16=False,\n",
    "    # (그래디언트 클리핑을 위한 최대 노름 값 설정, 0.1 ~ 10)\n",
    "    # 그래디언트 클리핑은 그래디언트의 크기가 너무 커서 학습이 불안정해지는 것을 방지하기 위한 기법 0.3 초과하지 않도록 함 (크면 가중치가 너무 크게업데이트됨)\n",
    "    max_grad_norm=0.3,\n",
    "    # 전체 학습단계 수 (-1 인 경우 epochs, 이외에는 스탭수,  -1~100000)\n",
    "    max_steps=-1,\n",
    "    # 학습률 warmup비율 설정 (학습 초기에 학습률을 서서히 증가시켜 안정적인 학습, 0.0~0.5)\n",
    "    # 너무 높으면 학습률이 너무 늦어짐\n",
    "    warmup_ratio=0.03,\n",
    "    # 배치 내 시쿼스 길이를 그룹화하여 패딩을 최소화 (데이터 시권스길이 유사한것 끼리 그룹화하여 메모리 효율설 높임)\n",
    "    group_by_length=True,\n",
    "    # 학습률 스케줄러의 유형 설정 (\"linear\",\"cosine\",\"constant\",\"polynomial\" 등) * constant는 학습률을 일정하게 유지\n",
    "    lr_scheduler_type=\"constant\",\n",
    "    # 학습 로그를 보고할 플랫폼 설정 (아웃풋 디렉토리 참고) -> tensorboard --logdir=./results/runs * wandb로 설정 가능\n",
    "    report_to=\"tensorboard\"\n",
    ")\n",
    "\n",
    "\"\"\"# 6. Train Model\"\"\"\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    # 학습할 모델\n",
    "    model=model,\n",
    "    # 모델 학습에 사용할 데이터셋\n",
    "    train_dataset=datasetCommon,\n",
    "    #  PEFT(파라미터 효율적 미세 조정) 설정 정의\n",
    "    peft_config=peft_params,\n",
    "    # 데이터셋에서 학습 데이터셋 텍스트 필드 이름\n",
    "    # dataset_text_field=\"text\",\n",
    "    # 입력 시퀀스 최대 길이 (128 ~ 1024) * 길이가 길수록 더많은 컨텍스트를 모델에 제공가능 단, 메모리 사용량 증가\n",
    "    # max_seq_length=None,\n",
    "    # 모델과 함께 사용할 토크나이저\n",
    "    # tokenizer=tokenizer,\n",
    "    args=training_params,\n",
    "    # 입력 시퀀스 패킹여부 (패킹 시 짧고/긴 시퀀스를 혼합하여 배치 처리 서능 개선)\n",
    "    # packing=False,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5427d943-dcbc-449e-86bb-736b8c45fece",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025_07_18_16\n"
     ]
    }
   ],
   "source": [
    "now_str = datetime.now().strftime(\"%Y_%m_%d_%H\")\n",
    "print(now_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d4e854d4-f807-4436-87c6-ca42d9e4e9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if inColab == True:\n",
    "    savePath = f\"/content/gdrive/MyDrive/Colab Notebooks/models/gemma_tune_{now_str}\"\n",
    "    trainer.save_model(savePath)\n",
    "else:\n",
    "    savePath = f\"./models/gemma_tune_{now_str}\"\n",
    "    trainer.save_model(savePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68234e07-a392-436e-9179-70fbb29d760b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a80632a-ad12-4841-8b07-c2cb740242a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
